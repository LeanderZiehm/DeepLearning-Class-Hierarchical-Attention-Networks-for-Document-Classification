# General
Hirachial Structure. + Attention. 

# GRU
GRU vs LSTM:
GRU is simpler and faster, good for short context.
Bidirectional GRU with Dropout 0.1 might be a bit low 0.2 to 0.5 might have been better to fight against overfitting.

CrossEntropyLoss vs  nn.NLLLoss negative log likelihood


# Optimizer:
Adam: step size 5, gamma 0.5, weight_decay 1e-5

# Regularizaion 
L2 L1 Dropout Early Stoppig Data augmentaion



# steps
word encoder
bi directional gru 
word attention layer
attention weights 
sentence encoder 

context vector.
document vector.




